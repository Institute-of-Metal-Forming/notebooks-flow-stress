{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Modelling Flow Stress Using an Artificial Neural Network (ANN)\n",
    "\n",
    "Import of required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # for data loading\n",
    "import numpy as np  # for vectorized computations\n",
    "import scipy.optimize as opt  # for least squares fitting\n",
    "import plotly.express as px  # for plotting\n",
    "from sklearn.neural_network import MLPRegressor  # providing neural network logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Loading the Experimental Data\n",
    "\n",
    "We have the flow stress data in a long format CSV file and load it via `pandas` into a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    \"bst.csv\",  # file name\n",
    "    encoding=\"utf8\",  # use unicode to be safe on Windows systems (default on UNIX)\n",
    "    sep=\",\",  # columns separated by comma\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "As the data is in long format (columns for temperature, strain and strain rate; rows for every data point), it is very comfortable to plot it with `plotly`.\n",
    "But first we sort by temperature, then by strain rate and last by strain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(\n",
    "    by=[\"temp\", \"rate\", \"strain\"],  # multi-level sorting columns\n",
    "    ascending=True,  # order from small to large\n",
    "    inplace=True,  # use the existing data frame, do not copy the data\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Now we can plot the data in dependence of the test conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(\n",
    "    data,\n",
    "    \"strain\",  # strain on the x-axis\n",
    "    \"stress\",  # stress on the y-axis\n",
    "    color=\"temp\",  # distinguish temperatures by colors\n",
    "    facet_col=\"rate\",  # draw multiple plots for distinct strain rates\n",
    "    line_group=\"file\",  # distinguish lines by file name, avoid zick-zack curves\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Normalize the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Artificial neural networks are sensitive to input and output scaling. Therefore, it is recommended to scale the data to a fixed range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "First, we calculate the minimum and maximum values of the data for all columns. Then, we store their difference additonally in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = data.drop(\"file\", axis=\"columns\").aggregate([\"max\", \"min\"]).T\n",
    "scales[\"range\"] = scales[\"max\"] - scales[\"min\"]\n",
    "scales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "We define utitlity functions that do the scaling using the values computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(d):\n",
    "    normalized = d.copy()\n",
    "    normalized[scales.index] = (d[scales.index] - scales[\"min\"]) / scales[\"range\"]\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def denormalize(d):\n",
    "    denormalized = d.copy()\n",
    "    denormalized[scales.index] = d[scales.index] * scales[\"range\"] + scales[\"min\"]\n",
    "    return denormalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "The normalized experimental data is obtain by applying the `normalize()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = normalize(data)\n",
    "normalized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Defining the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "The `MLPRegressor` class provides the neural network for us. It has a variety of options to alter the network construction and fitting behavior. See the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor) for more information. Here, we only set the count and size (neurons per layer) of the hidden layers. The size of input and output layer is determined later when fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPRegressor(\n",
    "    hidden_layer_sizes=[100, 100, 100],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Fit the Model to the Data\n",
    "\n",
    "The model object has a method `fit()` which takes the input and output data to fit on as multi-dimensional arrays.\n",
    "The fit is directly stored within the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    normalized_data[[\"strain\", \"temp\", \"rate\"]].to_numpy(),\n",
    "    normalized_data[\"stress\"].to_numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Plot the Model Predictions Counter the Data\n",
    "\n",
    "First we create a fine cartesian raster to evaluate the model on, so we get smooth curves of the model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "strains = np.linspace(0, 1.2, 50)  # strain with 50 points between 0 and 1.2\n",
    "temps = (\n",
    "    data.temp.unique()\n",
    ")  # take only the distinct temperatures that are present in the data\n",
    "rates = data.rate.unique()  # respectively\n",
    "\n",
    "grid = pd.MultiIndex.from_product(\n",
    "    [temps, rates, strains], names=[\"temp\", \"rate\", \"strain\"]\n",
    ")\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "The raster is an index for a data frame, but we want it as an actual dataframe for easier computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = pd.DataFrame(index=grid).reset_index()\n",
    "model_data[\"stress\"] = pd.NA\n",
    "model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "Again, we have to normalize this raster to be able to evaluate the fitted model on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_model_data = normalize(model_data)\n",
    "normalized_model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Then we apply the fitted model with its `predict()` method and save the results in an additonal column in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_model_data[\"stress\"] = model.predict(\n",
    "    normalized_model_data[[\"strain\", \"temp\", \"rate\"]].to_numpy()\n",
    ")\n",
    "normalized_model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Now we can denormalize the model results to get the actual predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = denormalize(normalized_model_data)\n",
    "model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "We can plot the results as before with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(model_data, \"strain\", \"stress\", color=\"temp\", facet_col=\"rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "To plot both in comparison, we first have to merge the data frames. We distinguish model predictions and experimental data by an additonal column containg a marker label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.concat(\n",
    "    [model_data, data],  # list of the frames to combine\n",
    "    keys=[\"model\", \"exp\"],  # list of marker labels in same order as above\n",
    "    names=[\"type\", \"index\"],  # names of the marker column and the index column\n",
    ")\n",
    "combined_data.reset_index(\n",
    "    level=0, inplace=True\n",
    ")  # make the type column a normal column (was an index column)\n",
    "combined_data.fillna(\n",
    "    value={\"file\": \"\"}, inplace=True\n",
    ")  # fill missing file in model results\n",
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "Now we plot the data as before, but distinguish the origin by lime style (solid and dashed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(\n",
    "    combined_data,\n",
    "    \"strain\",\n",
    "    \"stress\",\n",
    "    color=\"temp\",\n",
    "    facet_col=\"rate\",\n",
    "    line_dash=\"type\",\n",
    "    line_group=\"file\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
